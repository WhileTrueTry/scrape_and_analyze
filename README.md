# Scrap & Analyze: Herramienta para el an√°lisis de contenido web

Este repositorio contiene un script de Python para automatizar el proceso de _scraping_ de sitios web y el an√°lisis de su contenido mediante Grandes Modelos de Lenguaje (LLMs). Utiliza una estrategia MapReduce para procesar grandes vol√∫menes de texto, garantizando un an√°lisis profundo y coherente incluso en sitios con mucho contenido.

El sistema es resistente a fallos, con reintentos inteligentes y rotaci√≥n autom√°tica de modelos LLM para maximizar la tasa de √©xito.
Esta herramienta se basa en las utilidades ofrecidas por la librer√≠a crawl4AI y el servicio de Groq Cloud, que ofrece acceso a LLMs de c√≥digo abierto en la nube mediante una API gratuita, o utilizando planes de pago.
- [P√°gina web de Crawl4AI](https://docs.crawl4ai.com/)
- [P√°gina web de Groq Cloud](https://console.groq.com/home)

## ‚ú® Caracter√≠sticas Principales

-   **Manejo Robusto de Errores**: Implementa una l√≥gica de reintentos con _exponential backoff_ para gestionar errores de API y l√≠mites de tasa (_rate limiting_).
-   **Rotaci√≥n Autom√°tica de Modelos**: Si un modelo LLM falla repetidamente o requiere un tiempo de espera excesivo, el sistema cambia autom√°ticamente al siguiente modelo de una lista predefinida para no interrumpir el proceso. Esta lista de puede redefinir como par√°metro de entrada
-   **Prompts Personalizables**: Permite definir plantillas de prompts espec√≠ficas para la fase de an√°lisis parcial y para la consolidaci√≥n final, adapt√°ndose a cualquier necesidad de extracci√≥n de informaci√≥n.
-   **Divisi√≥n Inteligente de Texto**: El texto se divide en fragmentos (chunks) que respetan los l√≠mites de tokens del modelo definidos por el usuario, con un solapamiento configurable entre p√°rrafos para no perder contexto.
-   **Configuraci√≥n Flexible**: M√∫ltiples par√°metros ajustables, como l√≠mites de tokens, n√∫mero de p√°ginas a recolectar, profundidad de b√∫squeda, tiempos de espera, modelos a utilizar, etc...
-   **Modularidad**: Separaci√≥n clara entre la l√≥gica de scraping (presente en un m√≥dulo `scraper.py`) y la l√≥gica de an√°lisis.

## ‚öôÔ∏è ¬øC√≥mo Funciona?

<img src="diagrama_flujo_ScrapAndAnalyze.png" alt="Diagrama del flujo" width="1000"/>

El proceso sigue un flujo de trabajo claro y efectivo:

1.  **Scraping (Extracci√≥n)**: Se utiliza un m√≥dulo externo (`scraper.py`), basado en la librer√≠a crawl4ai, para navegar y extraer el contenido de las p√°ginas de un sitio web, limpi√°ndolo y convirti√©ndolo a formato Markdown. Para m√°s informaci√≥n sobre este m√≥dulo, o para usarlo de modo independiente, puede referirse a la [documentaci√≥n del scraper](https://github.com/WhileTrueTry/crawler-wrapper). El tipo de sitios que pueden ser scrapeados con esta herramienta sencilla est√° limitado a p√°ginas web que no se esfuerzan por evitarlo.
2.  **Divisi√≥n (Chunking)**: El contenido de todas las p√°ginas se combina y se divide en fragmentos de texto m√°s peque√±os (`chunks`), asegurando que cada uno quepa en el contexto del LLM y manteniendo un solapamiento para la coherencia.
3.  **An√°lisis Parcial**: Cada `chunk` se env√≠a al LLM junto con un "prompt parcial". El modelo extrae la informaci√≥n relevante de ese fragmento espec√≠fico.
4.  **Consolidaci√≥n**: Todos los res√∫menes parciales se juntan y se env√≠an de nuevo al LLM con un "prompt final". En esta fase, el modelo tiene la tarea de unificar la informaci√≥n, eliminar redundancias y generar un informe final bien estructurado.
5.  **Reducci√≥n Recursiva**: Si el conjunto de res√∫menes parciales es demasiado grande para caber en un solo llamado al LLM, el proceso de consolidaci√≥n se repite en capas hasta que toda la informaci√≥n se pueda procesar en un solo paso final.

## üìã Prerrequisitos

-   Python 3.8 o superior.
-   Una clave de API de [GroqCloud](https://console.groq.com/keys).

```bash
pip install crawl4ai bs4 tiktoken langchain_groq python-dotenv
```


## üöÄ Instalaci√≥n y Configuraci√≥n

1.  **Clona el repositorio:**
    ```bash
    git clone https://github.com/tu-usuario/tu-repositorio.git
    cd tu-repositorio
    ```

2.  **Instala las dependencias:**
    Instalar dependencias:
    ```bash
    pip install -r requirements.txt
    ```

3.  **Configurar credenciales:**
    Crea un archivo llamado `.env` en la ra√≠z del proyecto y a√±ade tu clave de API de Groq:
    ```
    GROQ_API_KEY="gsk_tu_api_key_aqui"
    ```

    O asegurate de establecer la variable de entorno por l√≠nea de comandos
    

## üìÅ Estructura de archivos guardados

Cuando se especifican par√°metros para guardar los archivos scrapeados y/o el resumen producido, se los podr√° encontrar de este modo
`save_scraping` = True
`save_scraping_directory` = "mis_datos_scrapeados"
`save_summary` = True
`save_summary_directory` = "mis_resumenes"

```
[save_scraping_directory]/
‚îú‚îÄ‚îÄ 001_index_markdown.txt
‚îú‚îÄ‚îÄ 002_about_markdown.txt
‚îî‚îÄ‚îÄ 003_contact_markdown.txt

[save_summary_directory]/
‚îú‚îÄ‚îÄ organizacion_resumen.txt
```

## Usage Example

El script est√° dise√±ado para ser ejecutado desde su bloque `if __name__ == '__main__':` o importando la clase `ScrapAndAnalyze` en otro m√≥dulo. A continuaci√≥n se muestra un ejemplo completo de uso.

```python
import asyncio
from scrap_and_analyze import ScrapAndAnalyze 

# 1. Definir la estructura y los prompts para guiar al LLM
#    Este ejemplo busca extraer informaci√≥n institucional de una ONG.

ejemplo_estructura = """
Nombre: [Nombre de la organizaci√≥n]
Aspectos institucionales
    ‚Ä¢ Estatus legal: [Descripci√≥n]
    ‚Ä¢ A√±o de creaci√≥n: [A√±o]
    ‚Ä¢ Misi√≥n: [Texto de la misi√≥n]
    ‚Ä¢ Visi√≥n: [Texto de la visi√≥n]
    ‚Ä¢ Financiamiento: [Fuentes de financiamiento]
Producci√≥n
    ‚Ä¢ Tem√°ticas: [Lista de temas que aborda]
    ‚Ä¢ Productos: [Informes, campa√±as, etc.]
Comunicaci√≥n
    ‚Ä¢ P√°gina web: [URL]
    ‚Ä¢ Redes tem√°ticas: [Si participa en alguna]
Estructura institucional: [Tipo de estructura, ej: ONG, Fundaci√≥n, etc.]
"""

prompt_parcial = """
Tu tarea es analizar un fragmento de texto extra√≠do del sitio web de la organizaci√≥n "{organizacion}".
Extrae y resume la informaci√≥n m√°s relevante bas√°ndote en la estructura del siguiente ejemplo.
No inventes informaci√≥n. Si algo no aparece en el texto, no lo incluyas.
Este es solo un trozo del contenido total. C√©ntrate √∫nicamente en lo que encuentres en el texto proporcionado.

EJEMPLO DE ESTRUCTURA:
{ejemplo}

---
CONTENIDO DEL FRAGMENTO A ANALIZAR:
{contenido_chunk}
---

Extrae la informaci√≥n relevante de este fragmento:
"""

prompt_final = """
Eres un analista experto. Has recibido varios res√∫menes parciales extra√≠dos de diferentes p√°ginas del sitio web de la organizaci√≥n "{organizacion}" ({web_oficial}).
Tu objetivo es consolidar toda esta informaci√≥n en un √∫nico resumen final, bien estructurado, coherente y sin redundancias.
Usa el siguiente formato como gu√≠a. No uses formato Markdown, solo texto simple.

EJEMPLO DE ESTRUCTURA FINAL:
{ejemplo}

---
RES√öMENES PARCIALES A CONSOLIDAR:
{resumenes_parciales}
---

Bas√°ndote en los res√∫menes anteriores, crea el informe final y consolidado sobre la organizaci√≥n:
"""

# Funci√≥n principal as√≠ncrona
async def main():
    # 2. Crear una instancia de la clase
    analyzer = ScrapAndAnalyze()

    # 3. Configurar los prompts personalizados
    analyzer.set_prompts(
        partial_prompt=prompt_parcial,
        final_prompt=prompt_final,
        example=ejemplo_estructura
    )

    # 4. Ejecutar el an√°lisis
    # Este ejemplo realiza el scraping, analiza el contenido y guarda los resultados en disco.
    print("Iniciando an√°lisis de la organizaci√≥n...")
    await analyzer.analyze_organization(
        organization_name="Centro de Estudios Legales y Sociales (CELS)",
        url="https://www.cels.org.ar/web/",
        max_pages=50,  # Limita el n√∫mero de p√°ginas a scrapear
        max_depth=3,   # Profundidad m√°xima de enlaces a seguir
        save_scraping=True,
        save_scraping_directory="resultados/scraping_cels",
        save_summary=True,
        save_summary_directory="resultados/resumenes_cels",
        return_result=False # No necesitamos que devuelva el texto si lo estamos guardando
    )
    print("An√°lisis completado. Revisa la carpeta 'resultados'.")

# 5. Ejecutar el programa
if __name__ == '__main__':
    asyncio.run(main())
```


## üõ†Ô∏è Configuraci√≥n Avanzada

Puedes personalizar el comportamiento de la clase ScrapAndAnalyze al momento de instanciarla:
| Par√°metro             | Descripci√≥n                                                                                       | Valor por Defecto                                         |
|-----------------------|---------------------------------------------------------------------------------------------------|-----------------------------------------------------------|
| models                | Lista de modelos LLM a utilizar, en orden de preferencia.                                         | ['openai/gpt-oss-120b', 'openai/gpt-oss-20b', 'deepseek-r1-distill-llama-70b', 'qwen/qwen3-32b']      |
| token_limit           | L√≠mite m√°ximo de tokens por defecto.                                                    | 5000                                                      |
| chunk_size_target     | Tama√±o objetivo de tokens para cada fragmento de texto. Se usa para calcular el tama√±o real del chunk. | 4100                                                      |
| overlap_paragraphs    | N√∫mero de p√°rrafos que se solapar√°n entre chunks consecutivos para mantener el contexto.         | 2                                                         |
| safety_margin         | N√∫mero de tokens reservados para la respuesta del modelo, evitando exceder el l√≠mite total.      | 1250                                                      |
| wait_between_chunks   | Segundos de espera entre llamadas exitosas al LLM para evitar el rate limiting.                  | 20                                                        |
| max_wait_for_switch   | Tiempo m√°ximo (en segundos) que el script esperar√° por una sugerencia retry-after antes de cambiar de modelo. | 600 (10 minutos)                                         |
| temperature           | Par√°metro de creatividad del LLM. 0 para respuestas m√°s deterministas.                           | 0                                                         |


## Ejemplo de instancia con configuraci√≥n personalizada:

```python
analyzer = ScrapAndAnalyze(
    models=["qwen/qwen3-32b", "deepseek-r1-distill-llama-70b"],
    token_limit=8000,
    wait_between_chunks=5,
    temperature=0.1
)
```

    

## ‚ö†Ô∏è Consideraciones importantes

### Limitaciones t√©cnicas del scraper sobre el que se construye la herramienta
- Solo extrae contenido del mismo dominio
- No ejecuta JavaScript complejo
- Puede tener problemas con sitios que requieren autenticaci√≥n
- No maneja CAPTCHAs autom√°ticamente

### No olvides que los modelos LLMs pueden alucinar. 
- Los resultados tienden a ser cada vez mejores, pero la revisi√≥n humana suma un valor diferencial.

  

## üôè Agradecimientos

- [crawl4ai](https://github.com/unclecode/crawl4ai) - La librer√≠a principal de web crawling
- [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) - Para el parsing de HTML
- Comunidad de Python por las incre√≠bles herramientas disponibles
- [Groq Cloud](https://console.groq.com/home)
- [Langchain](https://www.langchain.com/)
- Comunidad de Python por las incre√≠bles herramientas disponibles


‚≠ê Si este proyecto te fue √∫til, ¬°considera darle una estrella en GitHub!

  